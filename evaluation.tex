\section{Evaluation}
\label{sec:evaluation}

In this section, we evaluate the set abstractions.  We aim to answer the following questions about set abstractions. Can set abstractions be sufficiently precise to be useful? Can precision be made available while providing scalability? What trade-offs are necessary to achieve scalability?  To evaluate these questions we implemented all of the aforementioned abstractions as an OCaml library and then evaluated the abstractions using three different sets of problems: (1) traces of set domain operations as used in Memcad to perform shape analysis in the presence of unstructured sharing (from \cite{memcad:15:sas}) totaling 4521 domain operations; (2) traces of set domain operations as used in JSAna to verify functions in selected JavaScript libraries (from \cite{hoo:14:sas,desync:15:esop}) totaling 23086 domain operations; and (3) the expressible subset of tests of the Python set data structure (as used for QUIC graphs~\cite{ab:ecoop:13}) totaling 207 lines of code.  Results are shown in Table~\ref{tab:results}.

\begin{table}[t]
    \caption{Number of proved properties ($\adomprove$), average aggregate run time for non-timed-out benchmarks (Time), and number of timed-out benchmarks (TO) for 24 Memcad benchmarks, 5 JSAna benchmarks, and 24 Python benchmarks.}
    \label{tab:results}
    \input{data/data_table}
\end{table}

Because the definition of necessary precision depends on the use of a domain, we measure precision by comparing against a standard for precision.  For Memcad, the linear set abstraction (\textbf{lin}) was designed to be as precise as is needed for the Memcad benchmarks.  This means that any abstraction that achieves the same number of proofs without timeout is sufficiently precise.  It is important to note that many of these proofs are not intended to succeed.  They are used as queries internally with the analysis, so it is not possible to achieve 100\%.  From the results, we see that all of the BDD-based abstractions (\textbf{bdd}) achieve this.  We can also see that the equality (eq) and packing (pack) functor, regardless of the order in which they are applied do not change precision when applied to the BDD.  However, when applied to the linear set abstraction, they do sometime change precision.  This is because they affect internal representation and potentially affect the heuristics used within the abstract domain.  QUIC graphs (\textbf{quic}) and SMT (\textbf{smt}) do not perform as well under any configuration.  The reason for this is that QUIC graphs does not employ appropriate heuristics for all of the cases needed by Memcad and both have performance problems that cause them to time out before completing some benchmarks.

For the JSAna benchmarks, the BDD abstraction was designed to meet its precision needs and adding the equality or packing functor do not affect precision in any way.  It only affects performance.  However, the linear sets abstraction is not able to cope with the non-disjoint-union constraints that arise frequently in the JSAna benchmarks and thus loses all precision rapidly.  By comparison, QUIC graphs performs well.  It is unable to prove as many properties as is needed by JSAna, but it is still able to prove many properties.  Once again, tuning the heuristics could improve this precision, but possibly at the cost of performance.  SMT, once again does not perform well because of performance problems.  On the benchmarks where it completes, it is identical in precision to BDDs.

The Python benchmarks are slightly different because they are an analysis of programs rather than traces of domain operations.  Each program contains a couple of properties to verify, so the target is 100\%.  Here we see that none of the abstractions are able to achieve 100\%.  The linear set abstraction cannot achieve this because it is unable to represent the non-disjoint-union constructs.  The BDD and SMT abstractions cannot achieve 100\% because they do not support full cardinality reasoning.  Once again, QUIC graphs is insufficient because of the limited heuristics it employs as well as some performance problems.

The scalability of the abstractions can be seen in Table~\ref{tab:results} in the total analysis time, which measures the time to run the full benchmark suite, on average.  The times are only directly comparable if there are no time outs, which happens after 60 seconds per benchmark.  We first see that the linear domain is reliably fast.  Applying the equality and packing functors generally does not affect performance significantly.  BDDs, by comparison are less reliable.  While in the Memcad benchmarks, they perform well, nearly matching the linear domain, in the JSAna benchmarks we see significant variability.  In fact, without any of the functors as in \cite{ab:ecoop:13}, performance can be unacceptably slow at almost 22 seconds to analyze five functions.  However, the addition of, in particular, the packing functor, makes a significant difference.  It lowers the cost of the analysis to a fraction of a second without losing any precision.  However, the variability here indicates that, depending on the particular benchmark (or, in fact, the BDD implementation), the optimum combination of functors may vary.  However, selecting the packing functor seems to be a benefit without significant risk.  The QUIC graphs performance is unreliable.  Due to the expensive pattern matching machinery, it does not compare in terms of performance, though it is helped significantly by the equality functor, at the cost of precision.  The SMT domain fails to perform, timing out on at least one test in each benchmark suite.  This is because the SMT solver is failing to perform incrementally.  In essence, it has the same workload as the BDD, but it performs these proofs lazily.  This laziness is not necessarily a problem if work can be reused from one proof to the next, but it appears that this is not the case right now.  We suspect that the combination of doing validity proofs (instead of satisfiability queries) with quantifiers is preventing this reuse.

The results make clear four things.  First, if it is possible to design a targeted abstraction as the linear abstraction is for Memcad, it is worth it.  The performance is reliable and the precision is predictable.  Second, if it is not clear what the constraints may be, BDDs provide a good alternative that gives excellent (if not perfect due to the insufficient cardinality reasoning) precision with the risk of less reliable performance.  Third, much of the risk can be eliminated through the use of functors.  For equality heavy loads, the equality functor provides a significant benefit.  The packing functor seems to reliably improve performance by simply lowering the cost of each BDD operation without any measurable impact on precision.  Lastly, unless the content-centric reasoning of QUIC graphs is necessary, it does not make sense to use it due to both unreliable performance and precision.  Similarly, with the current state of SMT, this is not an appropriate use.  It may be possible to fix this, but today, it remains impractical for performance reasons.


% want to show that set abstractions can scale.



% want to show expert system for deciding on set abstractions

% want to show that SMT has performance difficulties due to the laziness of the abstraction